{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f1f55-c93a-4cae-b0d8-c1c704527b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd6638-f8ee-4750-bdce-ef08646788ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a387d-af93-4801-9bfc-c6720709279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('dataset_big_patent_v3.json')\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7ffc9-6224-4123-9397-e785cfdc0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf93fec-7ed0-4b96-89a5-4238d7cb5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7bee6-0879-42df-8d81-a56d1454d8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('dataset_big_patent_v3.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.json_normalize(data, meta=['anchor', \"query\", \"positive\", \"negative\",])\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7e72d-3247-4c11-ae66-0add71325fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac3c7c-f3ea-4e49-96cc-05dafbf22a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(df.sample(n=10).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890317cf-2edf-441b-80b5-9b3eddea93dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6b0d5-cc27-4331-b6a2-be1e10af268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d7bf2-a67d-40fb-9386-8409d6cf8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "# text = \"The present disclosure relates to a disposable loading unit (DLU) or single use loading unit (SULU) for use with a surgical device\"\n",
    "\n",
    "# tokens = tokenizer(text, \n",
    "#                    padding=True, # Ajoute du padding pour uniformiser la longueur\n",
    "#                    truncation=True, # Tronque les séquences trop longues\n",
    "#                    return_tensors=\"pt\") # Retourne des tenseurs PyTorch\n",
    "\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16375cb-7c37-4640-b0ef-0b08c38bb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "val_size = 0.125\n",
    "\n",
    "train_df, test_df = train_test_split(df, \n",
    "# 80% train, 20% test                              test_size=test_size,\n",
    "                                     random_state=39)\n",
    "train_df, val_df = train_test_split(train_df, \n",
    "                                     test_size=val_size/(1-test_size),\n",
    "                                     random_state=39)\n",
    "# 70% train, 10% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91da35-aa44-438b-b90f-53427ec7fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"abstract\"], truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94cf1e8-3b61-4490-bc12-8fb0226d1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, texts, tokenizer, max_length):\n",
    "        self.texts = []\n",
    "        for column in column_names:\n",
    "            self.texts.extend(dataframe[column].astype(str).tolist())  # Ajoute le texte de chaque colonne à la liste\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(text,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=self.max_length,\n",
    "                                   return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': encoding['input_ids'].flatten()  # Pour l'auto-encoding\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5073e6d-fd2b-4e7c-9618-606e007d8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nom des colonnes à tokeniser\n",
    "column_names = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df730f5a-4dc7-4633-9288-043e55e2c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df, tokenizer, 512, column_names)\n",
    "val_dataset = CustomDataset(val_df, tokenizer, 512, column_names)\n",
    "test_dataset = CustomDataset(test_df, tokenizer, 512, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e67a60-6978-4ec7-b83b-2e8e18eb9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa1691-ce5f-4389-a3f4-a06f4533988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",          # Dossier pour sauvegarder les résultats\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=3,              # Nombre d'epochs\n",
    "#     per_device_train_batch_size=16,  # Taille du batch par GPU\n",
    "#     per_device_eval_batch_size=64,   # Taille du batch pour l'évaluation\n",
    "#     save_steps=1000,                # Fréquence de sauvegarde des checkpoints\n",
    "#     save_total_limit=2,              # Nombre maximum de checkpoints à conserver\n",
    "#     prediction_loss_only=True,       # Optimise seulement la loss de prédiction\n",
    "#     eval_steps=500,                  # Fréquence d'évaluation\n",
    "#     learning_rate=5e-5,             # Taux d'apprentissage\n",
    "#     weight_decay=0.01,               # Décroissance du poids\n",
    "# )\n",
    "\n",
    "# # 5. Entraînement du modèle\n",
    "# model = AutoModel.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# # 6. Sauvegarde du modèle fine-tuned\n",
    "# model.save_pretrained(\"./bge-m3-fine-tuned\")\n",
    "# tokenizer.save_pretrained(\"./bge-m3-fine-tuned\")\n",
    "\n",
    "# print(\"Entraînement terminé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9d7b1-4d27-4d15-b485-8249ebcbc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# import pandas as pd\n",
    "\n",
    "# # 1. Préparation des données\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, texts, tokenizer, max_length):\n",
    "#         self.texts = texts\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = str(self.texts[idx])\n",
    "#         encoding = self.tokenizer(text,\n",
    "#                                    truncation=True,\n",
    "#                                    padding='max_length',\n",
    "#                                    max_length=self.max_length,\n",
    "#                                    return_tensors='pt')\n",
    "#         return {\n",
    "#             'input_ids': encoding['input_ids'].flatten(),\n",
    "#             'attention_mask': encoding['attention_mask'].flatten()\n",
    "#         }\n",
    "\n",
    "# # 2. Chargement du modèle et du tokenizer\n",
    "# model_name = \"BAAI/bge-m3\"  # Remplace par le nom exact du modèle\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# # 3. Préparation du dataset\n",
    "# # Charge tes données à partir d'un fichier (CSV, JSON, etc.)\n",
    "# data = pd.read_json('dataset_big_patent_v3.json')\n",
    "# data = data.dropna(subset=['abstract'])\n",
    "# texts = data['abstract'].tolist()  # Remplace 'text_column' par le nom de ta colonne de texte\n",
    "# max_length = 512  # Adapte la longueur maximale des séquences\n",
    "\n",
    "# dataset = CustomDataset(texts, tokenizer, max_length)\n",
    "\n",
    "# # 4. Configuration de l'entraînement\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",          # Dossier pour sauvegarder les résultats\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=3,              # Nombre d'epochs\n",
    "#     per_device_train_batch_size=16,  # Taille du batch par GPU\n",
    "#     save_steps=1000,                # Fréquence de sauvegarde des checkpoints\n",
    "#     save_total_limit=2,              # Nombre maximum de checkpoints à conserver\n",
    "#     prediction_loss_only=True,       # Optimise seulement la loss de prédiction\n",
    "#     learning_rate=5e-5,             # Taux d'apprentissage\n",
    "#     weight_decay=0.01,               # Décroissance du poids\n",
    "# )\n",
    "\n",
    "# # 5. Entraînement du modèle\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# # 6. Sauvegarde du modèle fine-tuned\n",
    "# model.save_pretrained(\"./bge-m3-fine-tuned\")\n",
    "# tokenizer.save_pretrained(\"./bge-m3-fine-tuned\")\n",
    "\n",
    "# print(\"Entraînement terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fbca5-2066-46f8-a367-ed743f68a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelle piste\n",
    "# https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a033a-92be-496f-9369-68863bd4f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da054d5-a7fa-4b64-b3bf-536dda62ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd4fd3-1a33-4b17-916d-b9f687e969d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56006e06-f908-49c2-8139-53411cbda884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading datatset\n",
    "df = pd.read_json('dataset_big_patent_v3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c05f85-bf38-4b44-912c-5ef86e4e1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Bitsandbytes configuration\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d3321-0348-4ff2-a3cc-c08fa232c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ae79b-30bf-474d-a69c-ace77cbfb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Loading the Pre-Trained model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"intfloat/e5-mistral-7b-instruct\"\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      token=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c44e22-5cad-47f7-9e73-57389876ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tokenization with left padding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc09b7-04c6-4a07-b10a-19231e6476aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Test the model with zero-shopt inferencing\n",
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d71d9-bc9d-445d-8667-21250f2232c9",
   "metadata": {},
   "source": [
    "# Choses à faire demain\n",
    "\n",
    "1. jupyter notebook gpu acceleration -> Create venv Skipped\n",
    "2. https://www.datacamp.com/fr/tutorial/fine-tuning-large-language-models\n",
    "3. Savoir quelle colonne tokenizer\n",
    "4. Supervised learning -> Q&A # https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce6890-039d-4870-925e-57fa1cf55c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f53e8-c7c3-488e-b88e-03607ea8a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading datatset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('dataset_big_patent_v3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da17fa-8029-449b-b82b-bd6ef512608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "val_size = 0.125\n",
    "\n",
    "train_df, test_df = train_test_split(df, \n",
    "# 80% train, 20% test                              test_size=test_size,\n",
    "                                     random_state=39)\n",
    "train_df, val_df = train_test_split(train_df, \n",
    "                                     test_size=val_size/(1-test_size),\n",
    "                                     random_state=39)\n",
    "# 70% train, 10% validation\n",
    "\n",
    "train_df = train_df.copy()\n",
    "val_df = val_df.copy()\n",
    "test_df = test_df.copy()\n",
    "\n",
    "train_df['split'] = 'train'\n",
    "val_df['split'] = 'val'\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "df_final = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde8d2d-1b86-4159-8bb2-c2994b1d26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset_big_patent_v3.json\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "val_size = 0.125\n",
    "\n",
    "train_df, test_df = train_test_split(df,\n",
    "                                     test_size=test_size,\n",
    "                                     random_state=39)\n",
    "\n",
    "ds_train = Dataset.from_pandas(train_df)\n",
    "ds_test = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": ds_train,\n",
    "    \"test\": ds_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0684c-faab-4b9c-a997-a5f27566267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"anchor\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39873db2-98b2-4e63-b5ba-8a59e06f2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db92ed-34ef-423d-afd6-741cde2fc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc6670-a822-450e-a3b6-c62589ffe36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9408724-28bc-4d12-89ff-52a49bee7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA disponible :\", torch.cuda.is_available())\n",
    "print(\"Nombre de GPU :\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Nom du GPU :\", torch.cuda.get_device_name(0))\n",
    "    print(\"Version CUDA utilisée par PyTorch :\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdabe1-7734-40e7-a6fc-328c394707ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f955542-480f-4182-beed-d06427b9e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "# original_model = AutoModel.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1a7bc-2eab-4a60-be7c-08866fbd1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe7671-8a15-4af5-8a7a-8c0d99d6a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23951888-d5f8-4997-8eac-2c452f9a8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b1eae-d706-4020-b5ee-5a938650fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Niveau de log : DEBUG pour tout voir\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990dfa02-6326-477b-a169-d87ec282659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf51ea-2bed-408a-8568-686f0f4a6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8274be6-fa8b-4de1-8b47-2737d5520ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f2a28-8478-4940-a614-8d943e7721f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a767b1-7c33-4a3d-b710-4c3e1d346846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"test_trainer\",\n",
    "   #evaluation_strategy=\"epoch\",\n",
    "   per_device_train_batch_size=1,  # Reduce batch size here\n",
    "   per_device_eval_batch_size=1,    # Optionally, reduce for evaluation as well\n",
    "   gradient_accumulation_steps=4\n",
    "   )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=original_model,\n",
    "   args=training_args,\n",
    "   train_dataset=small_train_dataset,\n",
    "   eval_dataset=small_eval_dataset,\n",
    "   compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b89b8-b7c0-470e-9f95-b8b0aca6a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9119972c-2043-4cd8-a7ad-f1ac8966c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset_big_patent_v3.json\", split=\"train\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bfb2b068-6a02-48a5-83fa-fe69f751d801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>query</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RELATED APPLICATIONS This application claims t...</td>\n",
       "      <td>What are the key advantages and applications o...</td>\n",
       "      <td>The present technology introduces an innovativ...</td>\n",
       "      <td>The invention relates to the design and utilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RELATED APPLICATIONS This application claims t...</td>\n",
       "      <td>How does a magnetic energy harvester operate w...</td>\n",
       "      <td>The advanced energy accumulation equipment bei...</td>\n",
       "      <td>The invention relates to the design and utilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RELATED APPLICATIONS This application claims t...</td>\n",
       "      <td>How does an energy harvester operate without a...</td>\n",
       "      <td>The invention relates to the design and utilit...</td>\n",
       "      <td>The present technology introduces an innovativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BACKGROUND OF THE INVENTION I. Field of the In...</td>\n",
       "      <td>How can buffer blocks for ruminant animals be ...</td>\n",
       "      <td>The innovative technique pertains to mineral s...</td>\n",
       "      <td>The latest invention provides novel systems an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RELATED APPLICATION The present application cl...</td>\n",
       "      <td>What advancements does the described patent pr...</td>\n",
       "      <td>The current text discusses a novel mechanical ...</td>\n",
       "      <td>The present invention addresses various improv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATION [0001] ...</td>\n",
       "      <td>What is the role of Onjisaponin B in the treat...</td>\n",
       "      <td>Delineation of the effect of a botanical enhan...</td>\n",
       "      <td>This invention pertains to a unique compound f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATION [0001] ...</td>\n",
       "      <td>What is the mechanism by which Onjisaponin B e...</td>\n",
       "      <td>This invention pertains to a unique compound f...</td>\n",
       "      <td>Delineation of the effect of a botanical enhan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>RELATED APPLICATIONS [0001] This application i...</td>\n",
       "      <td>What are the advantages of the improved dental...</td>\n",
       "      <td>The present innovations focus on an enhanced s...</td>\n",
       "      <td>[0001] This document covers the detailed aspec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>FIELD [0001] The invention refers to a ventric...</td>\n",
       "      <td>What are the benefits and mechanisms of a nove...</td>\n",
       "      <td>The technology relates to an innovation in hem...</td>\n",
       "      <td>The present technology involves an advancement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>FIELD OF THE INVENTION The present invention r...</td>\n",
       "      <td>What is the process for isolating micrin from ...</td>\n",
       "      <td>The invention pertains to an endogenous compou...</td>\n",
       "      <td>The subject invention relates to novel peptide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                anchor  \\\n",
       "0    RELATED APPLICATIONS This application claims t...   \n",
       "1    RELATED APPLICATIONS This application claims t...   \n",
       "2    RELATED APPLICATIONS This application claims t...   \n",
       "3    BACKGROUND OF THE INVENTION I. Field of the In...   \n",
       "4    RELATED APPLICATION The present application cl...   \n",
       "..                                                 ...   \n",
       "494  CROSS-REFERENCE TO RELATED APPLICATION [0001] ...   \n",
       "495  CROSS-REFERENCE TO RELATED APPLICATION [0001] ...   \n",
       "496  RELATED APPLICATIONS [0001] This application i...   \n",
       "497  FIELD [0001] The invention refers to a ventric...   \n",
       "498  FIELD OF THE INVENTION The present invention r...   \n",
       "\n",
       "                                                 query  \\\n",
       "0    What are the key advantages and applications o...   \n",
       "1    How does a magnetic energy harvester operate w...   \n",
       "2    How does an energy harvester operate without a...   \n",
       "3    How can buffer blocks for ruminant animals be ...   \n",
       "4    What advancements does the described patent pr...   \n",
       "..                                                 ...   \n",
       "494  What is the role of Onjisaponin B in the treat...   \n",
       "495  What is the mechanism by which Onjisaponin B e...   \n",
       "496  What are the advantages of the improved dental...   \n",
       "497  What are the benefits and mechanisms of a nove...   \n",
       "498  What is the process for isolating micrin from ...   \n",
       "\n",
       "                                              positive  \\\n",
       "0    The present technology introduces an innovativ...   \n",
       "1    The advanced energy accumulation equipment bei...   \n",
       "2    The invention relates to the design and utilit...   \n",
       "3    The innovative technique pertains to mineral s...   \n",
       "4    The current text discusses a novel mechanical ...   \n",
       "..                                                 ...   \n",
       "494  Delineation of the effect of a botanical enhan...   \n",
       "495  This invention pertains to a unique compound f...   \n",
       "496  The present innovations focus on an enhanced s...   \n",
       "497  The technology relates to an innovation in hem...   \n",
       "498  The invention pertains to an endogenous compou...   \n",
       "\n",
       "                                              negative  \n",
       "0    The invention relates to the design and utilit...  \n",
       "1    The invention relates to the design and utilit...  \n",
       "2    The present technology introduces an innovativ...  \n",
       "3    The latest invention provides novel systems an...  \n",
       "4    The present invention addresses various improv...  \n",
       "..                                                 ...  \n",
       "494  This invention pertains to a unique compound f...  \n",
       "495  Delineation of the effect of a botanical enhan...  \n",
       "496  [0001] This document covers the detailed aspec...  \n",
       "497  The present technology involves an advancement...  \n",
       "498  The subject invention relates to novel peptide...  \n",
       "\n",
       "[499 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54481072-89a9-4981-93b0-a6496e42c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    texts = [f\"Context: {c}\\nQuestion: {q}\\nAnswer: {a}\" for c, q, a in zip(dataset[\"anchor\"], dataset[\"query\"], dataset[\"positive\"])]\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(texts, max_length=384, truncation=True, padding=\"max_length\")\n",
    "    # labels = input_ids\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74bb0fb1-9582-4b64-98f6-f215d13db7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6c41adb-32e0-4a19-b463-66e6d1f78d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-qa-finetune\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4, # TO TEST\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be4b0d4b-e57a-4473-8bb1-164e907e9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.learnpytorch.io/pytorch_cheatsheet/\n",
    "# Setup device-agnostic code \n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" # Apple GPU\n",
    "else:\n",
    "    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1bf393c2-0aa7-4194-8bb0-8264c9b88f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does the crowdsourcing method is used to adjust a video game element ?\n",
      "Answer: The crowdsourced method uses the data of other users and then creates a new version of the game. This can be done in several ways, including creating a new level that will have more interesting gameplay elements, or by adding new levels to an existing one\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot performance\n",
    "\n",
    "prompt = \"Question: How does the crowdsourcing method is used to adjust a video game element ?\\nAnswer:\" # Expected : A processor retrieves a plurality of received game element feedback data from a plurality of users of a game and causes the game element to be adjusted during execution of the game \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "72dcbb96-a260-4d32-a1f8-591c70a37dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # adapte selon le modèle\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94b4b267-9051-455f-b5af-8ddc3ccbae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e9c61-b6ce-4a50-9b43-9ffc353ba383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matts\\AppData\\Local\\Temp\\ipykernel_15304\\3724420681.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='257' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [257/372 1:10:38 < 31:51, 0.06 it/s, Epoch 2.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.145400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.084500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.874300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ba4e0-48b4-4756-b2ad-d2cf1b4892c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9cbe6-2969-454a-ba42-aa932642832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned performance after\n",
    "\n",
    "prompt = \"Question: How does the crowdsourcing method is used to adjust a video game element ?\\nAnswer:\" # Expected : A processor retrieves a plurality of received game element feedback data from a plurality of users of a game and causes the game element to be adjusted during execution of the game\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee22adc-ce04-4be9-823a-341920c050fe",
   "metadata": {},
   "source": [
    "# Conclusion et prochaines étapes\n",
    "\n",
    "1. Compare la performance zero-shot vs fine-tuning\n",
    "2. Si peu ou pas d’amélioration, il faut peut-être :\n",
    "\n",
    "Augmenter la taille ou la qualité du dataset.\n",
    "\n",
    "Ajuster le prompt pour le zero-shot.\n",
    "\n",
    "Continuer le fine-tuning avec plus d’époques ou de paramètres.\n",
    "\n",
    "---\n",
    "\n",
    "Prochaines étapes :\n",
    "Améliorer la qualité des prompts (prompt engineering) pour le zero-shot.\n",
    "\n",
    "Augmenter la taille ou la diversité du dataset pour le fine-tuning.\n",
    "\n",
    "Expérimenter avec différents hyperparamètres (learning rate, batch size, nombre d’époques).\n",
    "\n",
    "Data augmentation pour augmenter la variété.\n",
    "\n",
    "Validation croisée pour une meilleure généralisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
