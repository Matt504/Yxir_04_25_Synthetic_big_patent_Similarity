{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b33ffa1-12e6-4d6b-9ca7-6de3a91e075b",
   "metadata": {},
   "source": [
    "# Analyze dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f1f55-c93a-4cae-b0d8-c1c704527b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd6638-f8ee-4750-bdce-ef08646788ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a387d-af93-4801-9bfc-c6720709279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('dataset_big_patent_v3.json')\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7ffc9-6224-4123-9397-e785cfdc0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf93fec-7ed0-4b96-89a5-4238d7cb5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7bee6-0879-42df-8d81-a56d1454d8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open('dataset_big_patent_v3.json') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# df = pd.json_normalize(data, meta=['anchor', \"query\", \"positive\", \"negative\",])\n",
    "\n",
    "# # Display the DataFrame\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7e72d-3247-4c11-ae66-0add71325fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac3c7c-f3ea-4e49-96cc-05dafbf22a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Special display to see better\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(df.sample(n=10).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bfcc7-004c-43e7-8665-ec617d012ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5073e6d-fd2b-4e7c-9618-606e007d8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nom des colonnes à tokeniser\n",
    "column_names = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d71d9-bc9d-445d-8667-21250f2232c9",
   "metadata": {},
   "source": [
    "# Choses à faire demain\n",
    "\n",
    "1. jupyter notebook gpu acceleration -> Done for TF (useless) and Torch with Cuda\n",
    "2. https://www.datacamp.com/fr/tutorial/fine-tuning-large-language-models\n",
    "3. Savoir quoi tokenizer\n",
    "4. Supervised learning -> Q&A # https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc6670-a822-450e-a3b6-c62589ffe36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9408724-28bc-4d12-89ff-52a49bee7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA disponible :\", torch.cuda.is_available())\n",
    "print(\"Nombre de GPU :\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Nom du GPU :\", torch.cuda.get_device_name(0))\n",
    "    print(\"Version CUDA utilisée par PyTorch :\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b1eae-d706-4020-b5ee-5a938650fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Niveau de log : DEBUG pour tout voir\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990dfa02-6326-477b-a169-d87ec282659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf51ea-2bed-408a-8568-686f0f4a6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8274be6-fa8b-4de1-8b47-2737d5520ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f2a28-8478-4940-a614-8d943e7721f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b89b8-b7c0-470e-9f95-b8b0aca6a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9119972c-2043-4cd8-a7ad-f1ac8966c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset_big_patent_v3.json\", split=\"train\")\n",
    "\n",
    "# Split en train (80%) et test (20%)\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4be3a607-ccbb-488d-b44a-40cea7a45828",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    padding_side=\"left\", add_eos_token=True, add_bos_token=True, use_fast=False, # NOT TESTED\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "# A tester avec padding\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", trust_remote_code=True, padding_side=\"left\", add_eos_token=True, add_bos_token=True, use_fast=False)\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb2b068-6a02-48a5-83fa-fe69f751d801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>query</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RELATED APPLICATIONS This application claims t...</td>\n",
       "      <td>What are the key advantages and applications o...</td>\n",
       "      <td>The present technology introduces an innovativ...</td>\n",
       "      <td>The invention relates to the design and utilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RELATED APPLICATIONS This application claims t...</td>\n",
       "      <td>How does a magnetic energy harvester operate w...</td>\n",
       "      <td>The advanced energy accumulation equipment bei...</td>\n",
       "      <td>The invention relates to the design and utilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RELATED APPLICATIONS This application claims t...</td>\n",
       "      <td>How does an energy harvester operate without a...</td>\n",
       "      <td>The invention relates to the design and utilit...</td>\n",
       "      <td>The present technology introduces an innovativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BACKGROUND OF THE INVENTION I. Field of the In...</td>\n",
       "      <td>How can buffer blocks for ruminant animals be ...</td>\n",
       "      <td>The innovative technique pertains to mineral s...</td>\n",
       "      <td>The latest invention provides novel systems an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RELATED APPLICATION The present application cl...</td>\n",
       "      <td>What advancements does the described patent pr...</td>\n",
       "      <td>The current text discusses a novel mechanical ...</td>\n",
       "      <td>The present invention addresses various improv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATION [0001] ...</td>\n",
       "      <td>What is the role of Onjisaponin B in the treat...</td>\n",
       "      <td>Delineation of the effect of a botanical enhan...</td>\n",
       "      <td>This invention pertains to a unique compound f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATION [0001] ...</td>\n",
       "      <td>What is the mechanism by which Onjisaponin B e...</td>\n",
       "      <td>This invention pertains to a unique compound f...</td>\n",
       "      <td>Delineation of the effect of a botanical enhan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>RELATED APPLICATIONS [0001] This application i...</td>\n",
       "      <td>What are the advantages of the improved dental...</td>\n",
       "      <td>The present innovations focus on an enhanced s...</td>\n",
       "      <td>[0001] This document covers the detailed aspec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>FIELD [0001] The invention refers to a ventric...</td>\n",
       "      <td>What are the benefits and mechanisms of a nove...</td>\n",
       "      <td>The technology relates to an innovation in hem...</td>\n",
       "      <td>The present technology involves an advancement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>FIELD OF THE INVENTION The present invention r...</td>\n",
       "      <td>What is the process for isolating micrin from ...</td>\n",
       "      <td>The invention pertains to an endogenous compou...</td>\n",
       "      <td>The subject invention relates to novel peptide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                anchor  \\\n",
       "0    RELATED APPLICATIONS This application claims t...   \n",
       "1    RELATED APPLICATIONS This application claims t...   \n",
       "2    RELATED APPLICATIONS This application claims t...   \n",
       "3    BACKGROUND OF THE INVENTION I. Field of the In...   \n",
       "4    RELATED APPLICATION The present application cl...   \n",
       "..                                                 ...   \n",
       "494  CROSS-REFERENCE TO RELATED APPLICATION [0001] ...   \n",
       "495  CROSS-REFERENCE TO RELATED APPLICATION [0001] ...   \n",
       "496  RELATED APPLICATIONS [0001] This application i...   \n",
       "497  FIELD [0001] The invention refers to a ventric...   \n",
       "498  FIELD OF THE INVENTION The present invention r...   \n",
       "\n",
       "                                                 query  \\\n",
       "0    What are the key advantages and applications o...   \n",
       "1    How does a magnetic energy harvester operate w...   \n",
       "2    How does an energy harvester operate without a...   \n",
       "3    How can buffer blocks for ruminant animals be ...   \n",
       "4    What advancements does the described patent pr...   \n",
       "..                                                 ...   \n",
       "494  What is the role of Onjisaponin B in the treat...   \n",
       "495  What is the mechanism by which Onjisaponin B e...   \n",
       "496  What are the advantages of the improved dental...   \n",
       "497  What are the benefits and mechanisms of a nove...   \n",
       "498  What is the process for isolating micrin from ...   \n",
       "\n",
       "                                              positive  \\\n",
       "0    The present technology introduces an innovativ...   \n",
       "1    The advanced energy accumulation equipment bei...   \n",
       "2    The invention relates to the design and utilit...   \n",
       "3    The innovative technique pertains to mineral s...   \n",
       "4    The current text discusses a novel mechanical ...   \n",
       "..                                                 ...   \n",
       "494  Delineation of the effect of a botanical enhan...   \n",
       "495  This invention pertains to a unique compound f...   \n",
       "496  The present innovations focus on an enhanced s...   \n",
       "497  The technology relates to an innovation in hem...   \n",
       "498  The invention pertains to an endogenous compou...   \n",
       "\n",
       "                                              negative  \n",
       "0    The invention relates to the design and utilit...  \n",
       "1    The invention relates to the design and utilit...  \n",
       "2    The present technology introduces an innovativ...  \n",
       "3    The latest invention provides novel systems an...  \n",
       "4    The present invention addresses various improv...  \n",
       "..                                                 ...  \n",
       "494  This invention pertains to a unique compound f...  \n",
       "495  Delineation of the effect of a botanical enhan...  \n",
       "496  [0001] This document covers the detailed aspec...  \n",
       "497  The present technology involves an advancement...  \n",
       "498  The subject invention relates to novel peptide...  \n",
       "\n",
       "[499 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54481072-89a9-4981-93b0-a6496e42c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    texts = [f\"Context: {c}\\nQuestion: {q}\\nAnswer: {a}\" for c, q, a in zip(dataset[\"anchor\"], dataset[\"query\"], dataset[\"positive\"])]\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(texts, max_length=384, truncation=True, padding=\"max_length\")\n",
    "    # labels = input_ids\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74bb0fb1-9582-4b64-98f6-f215d13db7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089f8ebbffe34c7cb54abb84569c283c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5075d617d042da8ae64f0254180a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6c41adb-32e0-4a19-b463-66e6d1f78d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-qa-finetune\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4, # TO FINE TUNE\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be4b0d4b-e57a-4473-8bb1-164e907e9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# https://www.learnpytorch.io/pytorch_cheatsheet/\n",
    "# Setup device-agnostic code \n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" # Apple GPU\n",
    "else:\n",
    "    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bf393c2-0aa7-4194-8bb0-8264c9b88f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matts\\anaconda3\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does the crowdsourcing method is used to adjust a video game element ?\n",
      "Answer: Crowdsourced software development can be used to create new games. This is done by creating a community of people who want to contribute to the creation of a game. The players are not paid for their contributions, but they are compensated based on how well\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot performance\n",
    "\n",
    "prompt = \"Question: How does the crowdsourcing method is used to adjust a video game element ?\\nAnswer:\" # Expected : A processor retrieves a plurality of received game element feedback data from a plurality of users of a game and causes the game element to be adjusted during execution of the game \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72dcbb96-a260-4d32-a1f8-591c70a37dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8, # Rang des matrices LoRA : élevé-> adaptation grande mais aussi mémoire utilisée\n",
    "    lora_alpha=32, # Facteur de mise à l’échelle pour les matrices LoRA.\n",
    "    lora_dropout=0.05, # Evite l'overfitting\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # adapte selon le modèle\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94b4b267-9051-455f-b5af-8ddc3ccbae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "546e9c61-b6ce-4a50-9b43-9ffc353ba383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matts\\AppData\\Local\\Temp\\ipykernel_8196\\3979699078.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 1:57:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.354500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.243400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.946200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.972300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.934900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.092100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=372, training_loss=2.0596542153307187, metrics={'train_runtime': 7081.6916, 'train_samples_per_second': 0.211, 'train_steps_per_second': 0.053, 'total_flos': 1227202636087296.0, 'train_loss': 2.0596542153307187, 'epoch': 2.9779559118236474})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76cca475-571f-43b4-a4f5-3f4135c13243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qwen-qa-saved\\\\tokenizer_config.json',\n",
       " './qwen-qa-saved\\\\special_tokens_map.json',\n",
       " './qwen-qa-saved\\\\vocab.json',\n",
       " './qwen-qa-saved\\\\merges.txt',\n",
       " './qwen-qa-saved\\\\added_tokens.json',\n",
       " './qwen-qa-saved\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Sauvegarde du modèle fine-tuned\n",
    "model.save_pretrained(\"./qwen-qa-saved\")\n",
    "tokenizer.save_pretrained(\"./qwen-qa-saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f7ba4e0-48b4-4756-b2ad-d2cf1b4892c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9980943202972412,\n",
       " 'eval_runtime': 237.5271,\n",
       " 'eval_samples_per_second': 0.421,\n",
       " 'eval_steps_per_second': 0.421,\n",
       " 'epoch': 2.9779559118236474}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40a9cbe6-2969-454a-ba42-aa932642832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does the crowdsourcing method is used to adjust a video game element ?\n",
      "Answer: The crowdsourcing method is used in order to adapt a video game element. Crowdsourcing involves bringing together an audience of people who are interested and knowledgeable about a particular topic, such as the video game itself. In this case, the video game would\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned performance after\n",
    "\n",
    "prompt = \"Question: How does the crowdsourcing method is used to adjust a video game element ?\\nAnswer:\" # Expected : A processor retrieves a plurality of received game element feedback data from a plurality of users of a game and causes the game element to be adjusted during execution of the game\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
