{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b33ffa1-12e6-4d6b-9ca7-6de3a91e075b",
   "metadata": {},
   "source": [
    "# Analyse dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04468adb-a3ca-4ebf-b547-0ca9e5a55330",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbc6670-a822-450e-a3b6-c62589ffe36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9408724-28bc-4d12-89ff-52a49bee7402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible : True\n",
      "Nombre de GPU : 1\n",
      "Nom du GPU : NVIDIA GeForce GTX 1650 Ti\n",
      "Version CUDA utilisée par PyTorch : 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA disponible :\", torch.cuda.is_available())\n",
    "print(\"Nombre de GPU :\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Nom du GPU :\", torch.cuda.get_device_name(0))\n",
    "    print(\"Version CUDA utilisée par PyTorch :\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0b1eae-d706-4020-b5ee-5a938650fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Niveau de log : DEBUG pour tout voir\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990dfa02-6326-477b-a169-d87ec282659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8274be6-fa8b-4de1-8b47-2737d5520ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409f2a28-8478-4940-a614-8d943e7721f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 27 12:04:04 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.99                 Driver Version: 555.99         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   61C    P8              4W /   50W |     379MiB /   4096MiB |     35%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     13420    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     19196    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     23008    C+G   ...Brave-Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A     23432    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "415b89b8-b7c0-470e-9f95-b8b0aca6a126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\matts\\anaconda3\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\matts\\anaconda3\\lib\\site-packages (0.21.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\matts\\anaconda3\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2793c-000e-46a8-9d7d-264c290cecf0",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db51b0-a8ff-462d-8345-34f3c8a093e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f3b19-cddd-4eeb-9c8a-9f5c5f485925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('dataset_big_patent_v3.json')\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179e2d8-9454-48ac-953c-2fdc71965c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d6f97-15d0-4392-8160-a169d6ad10d0",
   "metadata": {},
   "source": [
    "2 versions :\n",
    "https://sbert.net/docs/sentence_transformer/loss_overview.html\n",
    "1. (anchor, positive, negative) triplets -> Ommission de query -> MultipleNegativesRankingLoss\n",
    "2. Create a cutom Loss function for quadruplet -> (anchor, query, positive, negative)\n",
    "\n",
    "Nouvelle idée : mix des deux triplets et une seule loss fonction pour les deux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89604d09-4cc2-4208-90ba-28c5a1a35adf",
   "metadata": {},
   "source": [
    "# V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d7e17f-1db9-4727-a23a-a05218b9bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\matts\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\matts\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5d83a81-2c25-445f-be2c-148a81d7ed38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Matts\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "\n",
    "# Load a model to finetune\n",
    "model = SentenceTransformer(\n",
    "    \"intfloat/multilingual-e5-large-instruct\", # Restart kernel if not working but it is a functionnal V0-> \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    trust_remote_code=True,\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4ea144f-057f-4dd4-bab4-9354d4b1ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"dataset_big_patent_v3.json\", split=\"train\")\n",
    "\n",
    "dataset = dataset.remove_columns('query')\n",
    "\n",
    "# Split en train (80%) et test (20%)\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eb81cc9-927c-4968-8d67-1fa8f4493dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# def back_translate(text, src_lang='en', tgt_lang='fr'):\n",
    "#     # Traduction source -> cible\n",
    "#     model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "#     tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "#     model = MarianMTModel.from_pretrained(model_name)\n",
    "#     translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
    "#     intermediate_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "#     # Source\n",
    "#     model_name = f'Helsinki-NLP/opus-mt-{tgt_lang}-{src_lang}'\n",
    "    \n",
    "#     tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "#     model = MarianMTModel.from_pretrained(model_name)\n",
    "#     back_translated = model.generate(**tokenizer(intermediate_text, return_tensors=\"pt\", padding=True))\n",
    "#     return tokenizer.decode(back_translated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa1a84b1-ab8d-4092-a680-3e9e7e973b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug in c:\\users\\matts\\anaconda3\\lib\\site-packages (1.1.11)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from nlpaug) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from nlpaug) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from nlpaug) (2.32.3)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\matts\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\matts\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\matts\\anaconda3\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f4bd2f8-7b4a-4470-94b6-e5ad561a0916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\matts\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba57403c-a988-43f2-8a05-7cb2e41039fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in c:\\users\\matts\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sacremoses) (2024.9.11)\n",
      "Requirement already satisfied: click in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\matts\\anaconda3\\lib\\site-packages (from sacremoses) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\matts\\anaconda3\\lib\\site-packages (from click->sacremoses) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bbf90f1-2528-4991-a49f-1841c8b1b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\matts\\anaconda3\\lib\\site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.11.0)\n",
      "Requirement already satisfied: hf-xet>=0.1.4 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (1.0.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\matts\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\matts\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdf07582-0075-4184-913b-021e564992fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Matts\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774da2cb-9572-4e04-b6c5-1e72c2d88ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation -> Too long\n",
    "\n",
    "# import nlpaug.augmenter.word as naw\n",
    "# from transformers import pipeline\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def augment_triplet(anchor, positive, negative):\n",
    "\n",
    "#     paraphraser = pipeline('text2text-generation', model='t5-base')\n",
    "#     positive_aug = paraphraser(positive)[0]['generated_text']\n",
    "    \n",
    "#     # Remplacer 30% des mots dans negative\n",
    "#     aug = naw.SynonymAug(aug_src='wordnet')\n",
    "#     negative_aug = aug.augment(negative)[0]\n",
    "    \n",
    "#     return (anchor, positive_aug, negative_aug)\n",
    "\n",
    "# augmented_data = []\n",
    "# for a, p, n in tqdm(zip(train_dataset[\"anchor\"], train_dataset[\"positive\"], train_dataset[\"negative\"])):\n",
    "#     augmented_data.append(augment_triplet(a, p, n))\n",
    "\n",
    "# dataset = Dataset.from_dict({\n",
    "#     \"anchor\": [t[0] for t in augmented_data],\n",
    "#     \"positive\": [t[1] for t in augmented_data],\n",
    "#     \"negative\": [t[2] for t in augmented_data]\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48ed9dd9-f2b3-4343-bb98-e708d1e3b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "660d3c8c-f8e5-49f1-939d-e197697f9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required\n",
    "    output_dir=\"models/big-patent-triplet\",\n",
    "    # Optional\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  \n",
    "    bf16=False,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"big-patent-triplet-V1\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6d88f92-17b2-45e8-80b4-e8e04f762c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.max_seq_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4861c3f2-fd2f-4d82-a0d8-7c162dda0829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'big-patent-dev_cosine_accuracy': 0.6399999856948853}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_evaluator = TripletEvaluator(\n",
    "    anchors=test_dataset[\"anchor\"],\n",
    "    positives=test_dataset[\"positive\"],\n",
    "    negatives=test_dataset[\"negative\"],\n",
    "    name=\"big-patent-dev\",\n",
    "    batch_size=2\n",
    ")\n",
    "dev_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daca8659-92db-4c79-9c4e-2a769f7de2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 1:08:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'V1_eval_triplet_cosine_accuracy': 0.7794486284255981}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "triplet_evaluator = TripletEvaluator(\n",
    "    anchors=train_dataset[\"anchor\"],\n",
    "    positives=train_dataset[\"positive\"],\n",
    "    negatives=train_dataset[\"negative\"],\n",
    "    name=\"V1_eval_triplet\"\n",
    ")\n",
    "triplet_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2211710-1f27-475b-bef2-bf92216093e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/big-patent-e5-large-triplet-V2/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95215d9a-d067-4215-9d23-fa3ae8ebe249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step :\n",
    "# Data augmentation : Trop long\n",
    "# fine tuning : Done -> 0.78 acc\n",
    "# changer de modèle : ? \n",
    "# Multi triplet : Out Of memory -> Modèle moins gourmand ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93734fa-f422-454d-a9a4-73f8d781d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "print(is_torch_bf16_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800622e-ee40-405d-8b1c-0de80db0977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/train-sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d728b37-6763-4275-b870-595e0ec4a10b",
   "metadata": {},
   "source": [
    "# V2 -> CUDA OUT OF MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28cf1568-e7f7-4cbd-8701-1be6748f1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\n",
    "from sentence_transformers.losses import CoSENTLoss, MultipleNegativesRankingLoss, SoftmaxLoss\n",
    "\n",
    "# Load a model to finetune\n",
    "model = SentenceTransformer(\n",
    "    \"intfloat/multilingual-e5-large-instruct\", # Restart kernel if not working but it is a functionnal V0-> \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    trust_remote_code=True,\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea8c41f1-6384-49df-84e4-444289efa6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Loadseveral Datasets to train with\n",
    "# (anchor, positive, negative)\n",
    "anchor_triplet_train = load_dataset(\"json\", data_files=\"dataset_big_patent_v3.json\", split=\"train\")\n",
    "# (query, positive, negative)\n",
    "query_triplet_train = load_dataset(\"json\", data_files=\"dataset_big_patent_v3.json\", split=\"train\")\n",
    "\n",
    "anchor_triplet_train = anchor_triplet_train.remove_columns('query')\n",
    "query_triplet_train = query_triplet_train.remove_columns('anchor')\n",
    "\n",
    "# Split en train (80%) et test (20%)\n",
    "def split(dataset) :\n",
    "    split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    test_dataset = split_dataset[\"test\"]\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset_anchor, test_dataset_anchor = split(anchor_triplet_train) \n",
    "train_dataset_query, test_dataset_query = split(query_triplet_train) \n",
    "\n",
    "# Combine all datasets into a dictionary with dataset names to datasets\n",
    "train_dataset = {\n",
    "    \"anchor-triplet\": train_dataset_anchor,\n",
    "    \"query-triplet\": train_dataset_query,\n",
    "}\n",
    "\n",
    "# Use a dictionary for the evaluation dataset too, or just use one dataset or none at all\n",
    "test_dataset  = {\n",
    "    \"anchor-triplet\": test_dataset_anchor,\n",
    "    \"query-triplet\": test_dataset_query,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ea487f8-60c0-45b8-ae56-cef46563fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load several loss functions to train with\n",
    "mnrl_loss = MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff61a37f-f092-4824-93e5-d8d52a5a534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import SequentialEvaluator\n",
    "\n",
    "# Create a mapping with dataset names to loss functions, so the trainer knows which loss to apply where\n",
    "# Note: You can also just use one loss if all your training/evaluation datasets use the same loss\n",
    "losses = {\n",
    "    \"anchor-triplet\": mnrl_loss,\n",
    "    \"query-triplet\": mnrl_loss,\n",
    "}\n",
    "\n",
    "# Evaluators\n",
    "\n",
    "dev_evaluator_1 = TripletEvaluator(\n",
    "    anchors=train_dataset_anchor[\"anchor\"],\n",
    "    positives=train_dataset_anchor[\"positive\"],\n",
    "    negatives=train_dataset_anchor[\"negative\"],\n",
    "    name=\"big-patent-dev\",\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "dev_evaluator_2 = TripletEvaluator(\n",
    "    anchors=train_dataset_query[\"query\"],\n",
    "    positives=train_dataset_query[\"positive\"],\n",
    "    negatives=train_dataset_query[\"negative\"],\n",
    "    name=\"big-patent-dev\",\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "# Combines evaluators\n",
    "dev_evaluator = SequentialEvaluator([dev_evaluator_1, dev_evaluator_2], \n",
    "                                    main_score_function=lambda scores: scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79923454-98fb-45c1-b63d-4bb5630f5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required\n",
    "    output_dir=\"models/big-patent-triplet\",\n",
    "    # Optional\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  \n",
    "    bf16=False,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"big-patent-triplet-V1\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66289564-d383-4ac8-9faa-94d8044159cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 17.23 GiB is allocated by PyTorch, and 606.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     evaluator\u001b[38;5;241m=\u001b[39mdev_evaluator,\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     11\u001b[0m anchor_triplet_evaluator \u001b[38;5;241m=\u001b[39m TripletEvaluator(\n\u001b[0;32m     12\u001b[0m     anchors\u001b[38;5;241m=\u001b[39mtest_dataset_anchor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     13\u001b[0m     positives\u001b[38;5;241m=\u001b[39mtest_dataset_anchor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     14\u001b[0m     negatives\u001b[38;5;241m=\u001b[39mtest_dataset_anchor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     15\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV1_eval_triplet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m anchor_triplet_evaluator(model)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2246\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2247\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2248\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2249\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2250\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2558\u001b[0m )\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2566\u001b[0m ):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3782\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3780\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:2450\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 17.23 GiB is allocated by PyTorch, and 606.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    loss=losses,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "anchor_triplet_evaluator = TripletEvaluator(\n",
    "    anchors=test_dataset_anchor[\"anchor\"],\n",
    "    positives=test_dataset_anchor[\"positive\"],\n",
    "    negatives=test_dataset_anchor[\"negative\"],\n",
    "    name=\"V1_eval_triplet\"\n",
    ")\n",
    "anchor_triplet_evaluator(model)\n",
    "\n",
    "query_triplet_evaluator = TripletEvaluator(\n",
    "    anchors=test_dataset_query[\"query\"],\n",
    "    positives=test_dataset_query[\"positive\"],\n",
    "    negatives=test_dataset_query[\"negative\"],\n",
    "    name=\"V1_eval_triplet\"\n",
    ")\n",
    "query_triplet_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50897e5d-eb31-451a-af38-0d9482c72ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save the trained model and optionally push it to the Hugging Face Hub\n",
    "model.save_pretrained(\"big-patent-e5-large-multi-dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
